{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d521989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from typing import Tuple, Union\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from environment import Environment\n",
    "from model import Network\n",
    "import config\n",
    "import copy\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c15f48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수들\n",
    "directiondict = {\n",
    "    'stay': 4, 'north': 0, 'south': 1, 'west': 2, 'east': 3\n",
    "}\n",
    "reverse_directiondict = {v: k for k, v in directiondict.items()}\n",
    "\n",
    "def get_possible_directions(obs, obs_agents, agent_idx, agents_not_exchangeable, agents_fixed):\n",
    "    directions = []\n",
    "    directions_pushed_agents = []\n",
    "    if obs[0][agent_idx][1][3, 4] == 0:\n",
    "        directions.append('north')\n",
    "    if obs[0][agent_idx][1][5, 4] == 0:\n",
    "        directions.append('south')\n",
    "    if obs[0][agent_idx][1][4, 3] == 0:\n",
    "        directions.append('west')\n",
    "    if obs[0][agent_idx][1][4, 5] == 0:\n",
    "        directions.append('east')\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1)\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if agent_value in agents_not_exchangeable or agent_value in agents_fixed:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def get_possible_directions_super(obs, obs_agents, agent_idx, agents_fixed):\n",
    "    directions = []\n",
    "    directions_pushed_agents = []\n",
    "    if obs[0][agent_idx][2][4, 4] == 1:\n",
    "        directions.append('north')\n",
    "    if obs[0][agent_idx][3][4, 4] == 1:\n",
    "        directions.append('south')\n",
    "    if obs[0][agent_idx][4][4, 4] == 1:\n",
    "        directions.append('west')\n",
    "    if obs[0][agent_idx][5][4, 4] == 1:\n",
    "        directions.append('east')\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1)\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if agent_value in agents_fixed:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def push_recursive(obs, obs_agents, agent_super, agents_fixed):\n",
    "    relayed_actions = []\n",
    "    agents_not_exchangeable = []\n",
    "\n",
    "    current_agent = agent_super\n",
    "    depth = 0\n",
    "\n",
    "    # 스택에 (현재 에이전트, 남은 방향들, depth) 저장\n",
    "    stack = []\n",
    "\n",
    "    while True:\n",
    "        # 가능한 방향들 계산\n",
    "        if current_agent == agent_super:\n",
    "            possible_directions = get_possible_directions_super(obs, obs_agents, current_agent, agents_fixed)\n",
    "        else:\n",
    "            possible_directions = get_possible_directions(obs, obs_agents, current_agent, agents_not_exchangeable, agents_fixed)\n",
    "\n",
    "        while not possible_directions:\n",
    "            if not stack:\n",
    "                # 백트래킹할 곳이 없으면 종료\n",
    "                text = 'nope'\n",
    "                return text\n",
    "\n",
    "            # 스택에서 이전 상태로 백트래킹\n",
    "            last_agent, last_possible_directions, last_depth = stack.pop()\n",
    "\n",
    "            # 남은 방향이 있다면 그 중 하나를 선택하고 진행\n",
    "            if last_possible_directions:\n",
    "                relayed_actions = relayed_actions[:last_depth]  # 이전 선택을 지우고 다시 선택\n",
    "                current_agent = last_agent\n",
    "                possible_directions = last_possible_directions\n",
    "                depth = last_depth\n",
    "            else:\n",
    "                # 백트래킹할 방향이 없으면 계속 백트래킹\n",
    "                possible_directions = []\n",
    "\n",
    "        # 랜덤으로 가능한 방향 중 하나 선택\n",
    "        choosen_action = random.choice(possible_directions)\n",
    "        possible_directions.remove(choosen_action)\n",
    "\n",
    "        relayed_actions.append((current_agent, choosen_action[0]))\n",
    "\n",
    "        # 더 이상 밀 에이전트가 없으면 종료\n",
    "        if choosen_action[1] is None:\n",
    "            break\n",
    "\n",
    "        # depth에 따른 agents_not_exchangeable 처리\n",
    "        if depth == 1:\n",
    "            agents_not_exchangeable = []\n",
    "        agents_not_exchangeable.append(current_agent)\n",
    "\n",
    "        # 스택에 현재 상태를 저장\n",
    "        stack.append((current_agent, possible_directions, depth))\n",
    "\n",
    "        # 다음 에이전트를 선택하고 루프를 계속\n",
    "        current_agent = choosen_action[1]\n",
    "        depth += 1\n",
    "\n",
    "    return relayed_actions\n",
    "\n",
    "\n",
    "def get_possible_directions_radiation(obs, obs_agents, center_coordinates, agent_idx, agents_fixed):\n",
    "    directions = []\n",
    "    directions_pushed_agents = []\n",
    "    if obs[0][agent_idx][1][3, 4] == 0:\n",
    "        directions.append('north')\n",
    "    if obs[0][agent_idx][1][5, 4] == 0:\n",
    "        directions.append('south')\n",
    "    if obs[0][agent_idx][1][4, 3] == 0:\n",
    "        directions.append('west')\n",
    "    if obs[0][agent_idx][1][4, 5] == 0:\n",
    "        directions.append('east')\n",
    "    \n",
    "    row_diff = center_coordinates[0] - obs[2][agent_idx][0]\n",
    "    col_diff = center_coordinates[1] - obs[2][agent_idx][1]\n",
    "\n",
    "    if row_diff < 0:  # 에이전트가 중앙보다 아래에 있으면 북쪽으로 이동 불가\n",
    "        if 'north' in directions:\n",
    "            directions.remove('north')\n",
    "    elif row_diff > 0:  # 에이전트가 중앙보다 위에 있으면 남쪽으로 이동 불가\n",
    "        if 'south' in directions:\n",
    "            directions.remove('south')\n",
    "    if col_diff < 0:  # 에이전트가 중앙보다 오른쪽에 있으면 서쪽으로 이동 불가\n",
    "        if 'west' in directions:\n",
    "            directions.remove('west')\n",
    "    elif col_diff > 0:  # 에이전트가 중앙보다 왼쪽에 있으면 동쪽으로 이동 불가\n",
    "        if 'east' in directions:\n",
    "            directions.remove('east')\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1)\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if agent_value in agents_fixed:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def push_recursive_radiation(obs, obs_agents, center_coordinates, agent_idx, agents_fixed):\n",
    "\n",
    "    relayed_actions = []\n",
    "    agents_not_exchangeable = []\n",
    "    \n",
    "    current_agent = agent_idx\n",
    "    depth = 0\n",
    "\n",
    "    # 스택에 (현재 에이전트, 남은 방향들, depth) 저장\n",
    "    stack = []\n",
    "\n",
    "    while True:\n",
    "        # 가능한 방향들 계산\n",
    "        if current_agent == agent_idx:\n",
    "            possible_directions = get_possible_directions_radiation(obs, obs_agents, center_coordinates, current_agent, agents_fixed)\n",
    "        else:\n",
    "            possible_directions = get_possible_directions(obs, obs_agents, current_agent, agents_not_exchangeable, agents_fixed)\n",
    "\n",
    "        while not possible_directions:\n",
    "            if not stack:\n",
    "                # 백트래킹할 곳이 없으면 종료'\n",
    "                return [(agent_idx, 'stay')]\n",
    "\n",
    "            # 스택에서 이전 상태로 백트래킹\n",
    "            last_agent, last_possible_directions, last_depth = stack.pop()\n",
    "\n",
    "            # 남은 방향이 있다면 그 중 하나를 선택하고 진행\n",
    "            if last_possible_directions:\n",
    "                relayed_actions = relayed_actions[:last_depth]  # 이전 선택을 지우고 다시 선택\n",
    "                current_agent = last_agent\n",
    "                possible_directions = last_possible_directions\n",
    "                depth = last_depth\n",
    "            else:\n",
    "                # 백트래킹할 방향이 없으면 계속 백트래킹\n",
    "                possible_directions = []\n",
    "\n",
    "        # 랜덤으로 가능한 방향 중 하나 선택\n",
    "        choosen_action = random.choice(possible_directions)\n",
    "        possible_directions.remove(choosen_action)\n",
    "\n",
    "        relayed_actions.append((current_agent, choosen_action[0]))\n",
    "\n",
    "        # 더 이상 밀 에이전트가 없으면 종료\n",
    "        if choosen_action[1] is None:\n",
    "            break\n",
    "\n",
    "        # depth에 따른 agents_not_exchangeable 처리\n",
    "        if depth == 1:\n",
    "            agents_not_exchangeable = []\n",
    "        agents_not_exchangeable.append(current_agent)\n",
    "\n",
    "        # 스택에 현재 상태를 저장\n",
    "        stack.append((current_agent, possible_directions, depth))\n",
    "\n",
    "        # 다음 에이전트를 선택하고 루프를 계속\n",
    "        current_agent = choosen_action[1]\n",
    "        depth += 1\n",
    "\n",
    "    return relayed_actions\n",
    "\n",
    "\n",
    "def get_possible_directions_not_deadlock(obs, obs_agents, agent_idx, agent_action, agents_fixed):\n",
    "    directions = [agent_action]\n",
    "    directions_pushed_agents = []\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1)\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if agent_value in agents_fixed:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def push_recursive_not_deadlock(obs, obs_agents, agent_idx, agent_action, agents_fixed):\n",
    "\n",
    "    relayed_actions = []\n",
    "    agents_not_exchangeable = []\n",
    "    \n",
    "    current_agent = agent_idx\n",
    "    depth = 0\n",
    "\n",
    "    # 스택에 (현재 에이전트, 남은 방향들, depth) 저장\n",
    "    stack = []\n",
    "\n",
    "    while True:\n",
    "        # 가능한 방향들 계산\n",
    "        if current_agent == agent_idx:\n",
    "            possible_directions = get_possible_directions_not_deadlock(obs, obs_agents, agent_idx, agent_action, agents_fixed)\n",
    "        else:\n",
    "            possible_directions = get_possible_directions(obs, obs_agents, current_agent, agents_not_exchangeable, agents_fixed)\n",
    "\n",
    "        while not possible_directions:\n",
    "            if not stack:\n",
    "                # 백트래킹할 곳이 없으면 종료'\n",
    "                return [(agent_idx, 'stay')]\n",
    "\n",
    "            # 스택에서 이전 상태로 백트래킹\n",
    "            last_agent, last_possible_directions, last_depth = stack.pop()\n",
    "\n",
    "            # 남은 방향이 있다면 그 중 하나를 선택하고 진행\n",
    "            if last_possible_directions:\n",
    "                relayed_actions = relayed_actions[:last_depth]  # 이전 선택을 지우고 다시 선택\n",
    "                current_agent = last_agent\n",
    "                possible_directions = last_possible_directions\n",
    "                depth = last_depth\n",
    "            else:\n",
    "                # 백트래킹할 방향이 없으면 계속 백트래킹\n",
    "                possible_directions = []\n",
    "\n",
    "        # 랜덤으로 가능한 방향 중 하나 선택\n",
    "        choosen_action = random.choice(possible_directions)\n",
    "        possible_directions.remove(choosen_action)\n",
    "\n",
    "        relayed_actions.append((current_agent, choosen_action[0]))\n",
    "\n",
    "        # 더 이상 밀 에이전트가 없으면 종료\n",
    "        if choosen_action[1] is None:\n",
    "            break\n",
    "\n",
    "        # depth에 따른 agents_not_exchangeable 처리\n",
    "        if depth == 1:\n",
    "            agents_not_exchangeable = []\n",
    "        agents_not_exchangeable.append(current_agent)\n",
    "\n",
    "        # 스택에 현재 상태를 저장\n",
    "        stack.append((current_agent, possible_directions, depth))\n",
    "\n",
    "        # 다음 에이전트를 선택하고 루프를 계속\n",
    "        current_agent = choosen_action[1]\n",
    "        depth += 1\n",
    "\n",
    "    return relayed_actions\n",
    "\n",
    "\n",
    "def get_sorted_agents(agent_groups, env):\n",
    "    super_agents = []\n",
    "    for set_of_agents in agent_groups:\n",
    "        agent_super = max(set_of_agents, key=lambda i: np.sum(np.abs(env.agents_pos[i] - env.goals_pos[i])))\n",
    "        super_agents.append(agent_super)\n",
    "\n",
    "    # 각 에이전트와 목표 사이의 거리 계산\n",
    "    agent_distances = [(agent, np.sum(np.abs(env.agents_pos[agent] - env.goals_pos[agent]))) for agent in super_agents]\n",
    "\n",
    "    # 거리를 기준으로 내림차순 정렬\n",
    "    sorted_agents = sorted(agent_distances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 정렬된 에이전트 ID 추출\n",
    "    sorted_agent_groups = [agent for agent, distance in sorted_agents]\n",
    "    return sorted_agent_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45c9da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트\n",
    "class gpt4pathfinding:\n",
    "    def detection(self, agents_state):\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 MAPF 문제에서 에이전트들의 데드락 여부 감지를 위해 호출된 관리자입니다. 당신은 에이전트들의 움직임을 통해 에이전트들이 각각 어떤 상태에 있는지 추론할 수 있는 능력을 가지고 있습니다.\"},\n",
    "            {\"role\": \"user\", \"content\":\n",
    "                f\"\"\"\n",
    "                당신은 에이전트들의 32번간의 action log를 통해 에이전트들의 데드락 여부를 확인하고, 만약 데드락이면 서로 얽힌 가능성이 높은 에이전트들을 묶고, 데드락에 대한 해결 방법을 제시해야 합니다.\n",
    "                \n",
    "                데드락으로 분류되는 상태들은 다음과 같습니다:\n",
    "                - not arrived인 상태로 이동하지 않는 상태\n",
    "                - not arrived인 상태로 32번의 행동을 보았을 때 의미있는 좌표의 변화를 나타내지 못하고 배회하는 상태\n",
    "\n",
    "                데드락으로 분류되지 않는 상태들은 다음과 같습니다:\n",
    "                - not arrived였지만 특정 시점에 arrived가 되어 계속 멈춰있는 상태\n",
    "                - 계속 not arrived이지만 일관성 있는 좌표의 변화를 나타내며 이동하는 상태\n",
    "\n",
    "                만약 데드락인 에이전트들이 서로 매우 가까이 있다면 서로 얽혔을 가능성이 높으므로, 그들을 묶으세요/\n",
    "\n",
    "                만약 에이전트가 독립으로 데드락이라면, \"prime\" 방법을 이용합니다.\n",
    "                서로 얽힌 에이전트들 중, goal이 현재 위치에서 8 이상 떨어져 있는 에이전트가 있다면, \"prime\" 방법을 이용합니다. 에이전트들의 거리가 가깝더라도, 거리가 먼 goal로 한 에이전트를 이동시켜서 문제를 단순화할 수 있다면, \"prime\" 방법을 이용합니다.\n",
    "                서로 얽힌 에이전트들 중, goal도 모두 현재 위치에 가까이 위치한다면, \"radiation\" 방법을 이용합니다.\n",
    "\n",
    "                아래는 각 에이전트의 32번의 action log입니다.\n",
    "\n",
    "                {agents_state}\n",
    "\n",
    "                설명을 생성하지 말고, 아래 형식의 JSON 결과값만을 반환하세요.\n",
    "\n",
    "                그 다음, 각 에이전트의 상태를 JSON 형식으로 반환하세요. 각 에이전트의 상태는 다음 형식을 따라야 합니다:\n",
    "                {{\n",
    "                    \"agent_id\": [서로 얽힌 가능성이 높은 <에이전트 ID>들],\n",
    "                    \"deadlock\": \"yes\" 또는 \"no\"\n",
    "                    \"solusion\": \"prime\" 또는 \"radiation\"\n",
    "                }}\n",
    "\n",
    "                결과값에서 중복되는 에이전트가 있어서는 안 됩니다.\n",
    "                \"deadlock\" 상태가 \"no\"인 경우에는 \"solution\"이 필요하지 않습니다.\n",
    "                \n",
    "                예시:\n",
    "                [\n",
    "                    {{\"agent_id\": [1, 24, 32], \"deadlock\": \"yes\", \"solusion\": \"prime\"}},\n",
    "                    {{\"agent_id\": [30], \"deadlock\": \"yes\", \"solusion\": \"prime\"}},\n",
    "                    {{\"agent_id\": [4, 5], \"deadlock\": \"yes\", \"solusion\": \"radiation\"}},\n",
    "                    {{\"agent_id\": [16], \"deadlock\": \"no\"}}\n",
    "                    {{\"agent_id\": [20], \"deadlock\": \"no\"}}\n",
    "                ]\n",
    "                \"\"\"\n",
    "            }],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "pathfinder = gpt4pathfinding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "adf6f06d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(config.test_seed)\n",
    "np.random.seed(config.test_seed)\n",
    "random.seed(config.test_seed)\n",
    "DEVICE = torch.device('cpu')\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4af69b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test(test_env_settings: Tuple = config.test_env_settings, num_test_cases: int = config.num_test_cases):\n",
    "    '''\n",
    "    create test set\n",
    "    '''\n",
    "\n",
    "    for map_length, num_agents, density in test_env_settings:\n",
    "\n",
    "        name = f'./test_set/{map_length}length_{num_agents}agents_{density}density.pth'\n",
    "        print(f'-----{map_length}length {num_agents}agents {density}density-----')\n",
    "\n",
    "        tests = []\n",
    "\n",
    "        env = Environment(fix_density=density, num_agents=num_agents, map_length=map_length)\n",
    "\n",
    "        for _ in tqdm(range(num_test_cases)):\n",
    "            tests.append((np.copy(env.map), np.copy(env.agents_pos), np.copy(env.goals_pos)))\n",
    "            env.reset(num_agents=num_agents, map_length=map_length)\n",
    "        print()\n",
    "\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump(tests, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c11a84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_test():\n",
    "    env = Environment()\n",
    "    network = Network()\n",
    "    network.eval()\n",
    "    obs, last_act, pos = env.observe()\n",
    "    network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(pos.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b34b1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_range: Union[int, tuple], test_set=config.test_env_settings):\n",
    "    '''\n",
    "    test model in 'saved_models' folder\n",
    "    '''\n",
    "    network = Network()\n",
    "    network.eval()\n",
    "    network.to(DEVICE)\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count()//2)\n",
    "\n",
    "    if isinstance(model_range, int):\n",
    "        state_dict = torch.load(os.path.join(config.save_path, f'{model_range}.pth'), map_location=DEVICE)\n",
    "        network.load_state_dict(state_dict)\n",
    "        network.eval()\n",
    "        network.share_memory()\n",
    "\n",
    "        \n",
    "        print(f'----------test model {model_range}----------')\n",
    "\n",
    "        instance_id = 0\n",
    "\n",
    "        for case in test_set:\n",
    "            print(f\"test set: {case[0]} env {case[1]} agents\")\n",
    "            with open('./test_set/{}_{}agents.pth'.format(case[0], case[1]), 'rb') as f:\n",
    "                tests = pickle.load(f)\n",
    "\n",
    "            test = tests[0]\n",
    "            ret = test_one_case((test, network, instance_id))\n",
    "\n",
    "            success, steps, num_comm = ret\n",
    "\n",
    "            # instance_id_base = instance_id\n",
    "            # tests = [(test, network, instance_id_base + i) for i, test in enumerate(tests)]\n",
    "            # ret = pool.map(test_one_case, tests)\n",
    "\n",
    "            # success, steps, num_comm = zip(*ret)\n",
    "\n",
    "            # print(\"success rate: {:.2f}%\".format(sum(success)/len(success)*100))\n",
    "            # print(\"average step: {}\".format(sum(steps)/len(steps)))\n",
    "            # print(\"communication times: {}\".format(sum(num_comm)/len(num_comm)))\n",
    "            # print()\n",
    "\n",
    "            instance_id += len(tests)\n",
    "\n",
    "    elif isinstance(model_range, tuple):\n",
    "\n",
    "        for model_name in range(model_range[0], model_range[1]+1, config.save_interval):\n",
    "            state_dict = torch.load(os.path.join(config.save_path, f'{model_name}.pth'), map_location=DEVICE)\n",
    "            network.load_state_dict(state_dict)\n",
    "            network.eval()\n",
    "            network.share_memory()\n",
    "\n",
    "\n",
    "            print(f'----------test model {model_name}----------')\n",
    "\n",
    "            instance_id = 0\n",
    "\n",
    "            for case in test_set:\n",
    "                print(f\"test set: {case[0]} length {case[1]} agents {case[2]} density\")\n",
    "                with open(f'./test_set/{case[0]}length_{case[1]}agents_{case[2]}density.pth', 'rb') as f:\n",
    "                    tests = pickle.load(f)\n",
    "\n",
    "                test = tests[0]\n",
    "                ret = test_one_case((test, network, instance_id))\n",
    "\n",
    "                success, steps, num_comm = ret\n",
    "\n",
    "                # instance_id_base = instance_id\n",
    "                # tests = [(test, network, instance_id_base + i) for i, test in enumerate(tests)]\n",
    "                # ret = pool.map(test_one_case, tests)\n",
    "\n",
    "                # success, steps, num_comm = zip(*ret)\n",
    "\n",
    "                # print(\"success rate: {:.2f}%\".format(sum(success)/len(success)*100))\n",
    "                # print(\"average step: {}\".format(sum(steps)/len(steps)))\n",
    "                # print(\"communication times: {}\".format(sum(num_comm)/len(num_comm)))\n",
    "                # print()\n",
    "\n",
    "                instance_id += 1\n",
    "\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52ab1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_case(args):\n",
    "\n",
    "    env_set, network, instance_id = args\n",
    "\n",
    "    env = Environment()\n",
    "    env.load(np.array(env_set[0]), np.array(env_set[1]), np.array(env_set[2]))\n",
    "    obs, last_act, pos = env.observe()\n",
    "    \n",
    "    done = False\n",
    "    network.reset()\n",
    "\n",
    "    num_agents = len(env_set[1])\n",
    "\n",
    "    step = 0\n",
    "    num_comm = 0\n",
    "\n",
    "    while not done and env.steps < config.max_episode_length // 2:\n",
    "        actions, _, _, _, comm_mask = network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(pos.astype(int)))\n",
    "        (obs, last_act, pos), _, done, _ = env.step(actions)\n",
    "        env.save_frame(step, instance_id)\n",
    "        step += 1\n",
    "        num_comm += np.sum(comm_mask)\n",
    "\n",
    "    while not done and env.steps < config.max_episode_length:\n",
    "        env_copy = copy.deepcopy(env)\n",
    "        plan = []\n",
    "        not_arrived = set()\n",
    "        sim_obs, sim_last_act, sim_pos = env_copy.observe()\n",
    "        for _ in range(32):\n",
    "            if env_copy.steps >= config.max_episode_length:\n",
    "                break\n",
    "            actions, _, _, _, comm_mask = network.step(torch.as_tensor(sim_obs.astype(np.float32)).to(DEVICE), \n",
    "                                                        torch.as_tensor(sim_last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                        torch.as_tensor(sim_pos.astype(int)))\n",
    "            plan.append((actions, comm_mask, copy.deepcopy(sim_pos)))\n",
    "            (sim_obs, sim_last_act, sim_pos), _, sim_done, _ = env_copy.step(actions)\n",
    "            for i in range(num_agents):\n",
    "                if not np.array_equal(env_copy.agents_pos[i], env_copy.goals_pos[i]):\n",
    "                    not_arrived.add(i)\n",
    "        planned_steps_dict = {i: [] for i in not_arrived}\n",
    "        goal_logged = {i: False for i in not_arrived}\n",
    "        for i in plan:\n",
    "            actions, _, positions = i\n",
    "            for agent_idx in not_arrived:\n",
    "                position = positions[agent_idx]\n",
    "                # 목표 위치와 현재 위치를 비교하여 도달 여부 판단\n",
    "                arrived_status = \"Arrived\" if np.array_equal(position, env.goals_pos[agent_idx]) else \"Not arrived\"\n",
    "                direction = reverse_directiondict.get(actions[agent_idx], 'unknown')\n",
    "                if not goal_logged[agent_idx]:\n",
    "                    planned_steps_dict[agent_idx].append(\n",
    "                        f\"(Action: {direction}, Position: [{position[0]}, {position[1]}], {arrived_status})\"\n",
    "                    )\n",
    "                    goal_logged[agent_idx] = True  # 목표 위치 기록을 완료한 플래그 설정\n",
    "                else:\n",
    "                    planned_steps_dict[agent_idx].append(\n",
    "                        f\"(Action: {direction}, Position: [{position[0]}, {position[1]}], {arrived_status})\"\n",
    "                    )\n",
    "        # 결과를 n: [], m: [], o: [] 형식으로 출력\n",
    "        agents_state = \"\"\n",
    "        for agent_idx in planned_steps_dict:\n",
    "            agent_goal = f\" (Goal: [{env.goals_pos[agent_idx][0]}, {env.goals_pos[agent_idx][1]}])\"\n",
    "            agent_log = \", \".join(planned_steps_dict[agent_idx])\n",
    "            agents_state += f\"Agent {agent_idx}{agent_goal}: {agent_log}\\n\"\n",
    "        print(agents_state)\n",
    "        gpt4_response = pathfinder.detection(agents_state)\n",
    "        response_text = gpt4_response\n",
    "        print(response_text)\n",
    "        try:\n",
    "            start_idx = response_text.index('[')\n",
    "            end_idx = response_text.rindex(']') + 1\n",
    "            json_part = response_text[start_idx:end_idx]\n",
    "            json_data = json.loads(json_part)\n",
    "            print(\"Extracted JSON:\", json_data)\n",
    "        except:\n",
    "            print(\"JSON 부분을 찾을 수 없으므로 deadlock이 없다고 가정합니다.\")\n",
    "            json_data = []\n",
    "\n",
    "        deadlock_exists = any(item['deadlock'] == 'yes' for item in json_data)\n",
    "        \n",
    "        if not deadlock_exists:\n",
    "            for actions, comm_mask, _ in plan:\n",
    "                if env.steps >= config.max_episode_length:\n",
    "                    break\n",
    "                (obs, last_act, pos), _, done, _ = env.step(actions)\n",
    "                env.save_frame(step, instance_id)\n",
    "                step += 1\n",
    "                num_comm += np.sum(comm_mask)\n",
    "        else:\n",
    "            prime_agents = [item['agent_id'] for item in json_data if item.get('deadlock') == 'yes' and item.get('solution') == 'prime']\n",
    "            radiation_agents = [item['agent_id'] for item in json_data if item.get('deadlock') == 'yes' and item.get('solution') == 'radiation']\n",
    "            no_deadlock_agents = [item['agent_id'] for item in json_data if item.get('deadlock') == 'no']\n",
    "            sorted_prime_agents = get_sorted_agents(prime_agents, env)\n",
    "            sorted_no_deadlock_agents = get_sorted_agents(no_deadlock_agents, env)\n",
    "\n",
    "            for _ in range(16):\n",
    "                if env.steps >= config.max_episode_length:\n",
    "                    break\n",
    "                obs_agents = env.observe_agents()\n",
    "                observation = env.observe()\n",
    "\n",
    "                manual_actions = [4 for _ in range(num_agents)]\n",
    "                ml_planned_actions, _, _, _, _ = network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(pos.astype(int)))\n",
    "                \n",
    "                fixed_agents = []\n",
    "                for super_agent in sorted_prime_agents:\n",
    "                    for relayed_action in push_recursive(observation, obs_agents, super_agent, fixed_agents):\n",
    "                        manual_actions[relayed_action[0]] = directiondict[relayed_action[1]]\n",
    "                        fixed_agents.append(relayed_action[0])\n",
    "\n",
    "                for set_of_agents in radiation_agents:\n",
    "                    x_values = []\n",
    "                    y_values = []\n",
    "                    for agent_idx in set_of_agents:\n",
    "                        x_values.append(observation[2][agent_idx][0])\n",
    "                        y_values.append(observation[2][agent_idx][1])\n",
    "                    avg_x = sum(x_values) / len(x_values)\n",
    "                    avg_y = sum(y_values) / len(y_values)\n",
    "                    average_position = (avg_x, avg_y)\n",
    "\n",
    "                    for radiation_agent in set_of_agents:\n",
    "                        for relayed_action in push_recursive_radiation(observation, obs_agents, average_position, radiation_agent, fixed_agents):\n",
    "                            manual_actions[relayed_action[0]] = directiondict[relayed_action[1]]\n",
    "                            fixed_agents.append(relayed_action[0])\n",
    "                \n",
    "                for no_deadlock_agent in sorted_no_deadlock_agents:\n",
    "                    for relayed_action in push_recursive_not_deadlock(observation, obs_agents, no_deadlock_agent, reverse_directiondict[ml_planned_actions[no_deadlock_agent]], fixed_agents):\n",
    "                        manual_actions[relayed_action[0]] = directiondict[relayed_action[1]]\n",
    "                        fixed_agents.append(relayed_action[0])\n",
    "                        \n",
    "                (obs, last_act, pos), _, done, _ = env.step(manual_actions)\n",
    "                env.save_frame(step, instance_id)\n",
    "                step += 1\n",
    "                num_comm += np.sum(comm_mask)\n",
    "\n",
    "    return np.array_equal(env.agents_pos, env.goals_pos), step, num_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7da42859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_22800\\1337376553.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(config.save_path, f'{model_range}.pth'), map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------test model 128000----------\n",
      "test set: warehouse env 64 agents\n",
      "Agent 24 (Goal: [5, 157]): (Action: east, Position: [4, 116], Not arrived), (Action: east, Position: [4, 117], Not arrived), (Action: east, Position: [4, 118], Not arrived), (Action: east, Position: [4, 119], Not arrived), (Action: east, Position: [4, 120], Not arrived), (Action: east, Position: [4, 121], Not arrived), (Action: east, Position: [4, 122], Not arrived), (Action: east, Position: [4, 123], Not arrived), (Action: east, Position: [4, 124], Not arrived), (Action: east, Position: [4, 125], Not arrived), (Action: east, Position: [4, 126], Not arrived), (Action: east, Position: [4, 127], Not arrived), (Action: east, Position: [4, 128], Not arrived), (Action: east, Position: [4, 129], Not arrived), (Action: east, Position: [4, 130], Not arrived), (Action: east, Position: [4, 131], Not arrived), (Action: east, Position: [4, 132], Not arrived), (Action: east, Position: [4, 133], Not arrived), (Action: east, Position: [4, 134], Not arrived), (Action: east, Position: [4, 135], Not arrived), (Action: south, Position: [4, 136], Not arrived), (Action: east, Position: [5, 136], Not arrived), (Action: east, Position: [5, 137], Not arrived), (Action: east, Position: [5, 138], Not arrived), (Action: east, Position: [5, 139], Not arrived), (Action: east, Position: [5, 140], Not arrived), (Action: east, Position: [5, 141], Not arrived), (Action: east, Position: [5, 142], Not arrived), (Action: east, Position: [5, 143], Not arrived), (Action: east, Position: [5, 144], Not arrived), (Action: east, Position: [5, 145], Not arrived), (Action: east, Position: [5, 146], Not arrived)\n",
      "Agent 49 (Goal: [35, 4]): (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived), (Action: stay, Position: [34, 33], Not arrived)\n",
      "Agent 48 (Goal: [34, 38]): (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived), (Action: stay, Position: [34, 32], Not arrived)\n",
      "Agent 38 (Goal: [37, 126]): (Action: west, Position: [37, 130], Not arrived), (Action: west, Position: [37, 129], Not arrived), (Action: west, Position: [37, 128], Not arrived), (Action: west, Position: [37, 127], Not arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived), (Action: stay, Position: [37, 126], Arrived)\n",
      "\n",
      "[\n",
      "    {\"agent_id\": [24], \"deadlock\": \"no\"},\n",
      "    {\"agent_id\": [49, 48], \"deadlock\": \"yes\", \"solution\": \"prime\"},\n",
      "    {\"agent_id\": [38], \"deadlock\": \"no\"}\n",
      "]\n",
      "Extracted JSON: [{'agent_id': [24], 'deadlock': 'no'}, {'agent_id': [49, 48], 'deadlock': 'yes', 'solution': 'prime'}, {'agent_id': [38], 'deadlock': 'no'}]\n",
      "Agent 24 (Goal: [5, 157]): (Action: east, Position: [4, 132], Not arrived), (Action: east, Position: [4, 133], Not arrived), (Action: east, Position: [4, 134], Not arrived), (Action: east, Position: [4, 135], Not arrived), (Action: south, Position: [4, 136], Not arrived), (Action: east, Position: [5, 136], Not arrived), (Action: east, Position: [5, 137], Not arrived), (Action: east, Position: [5, 138], Not arrived), (Action: east, Position: [5, 139], Not arrived), (Action: east, Position: [5, 140], Not arrived), (Action: east, Position: [5, 141], Not arrived), (Action: east, Position: [5, 142], Not arrived), (Action: east, Position: [5, 143], Not arrived), (Action: east, Position: [5, 144], Not arrived), (Action: east, Position: [5, 145], Not arrived), (Action: east, Position: [5, 146], Not arrived), (Action: east, Position: [5, 147], Not arrived), (Action: east, Position: [5, 148], Not arrived), (Action: east, Position: [5, 149], Not arrived), (Action: east, Position: [5, 150], Not arrived), (Action: east, Position: [5, 151], Not arrived), (Action: east, Position: [5, 152], Not arrived), (Action: east, Position: [5, 153], Not arrived), (Action: east, Position: [5, 154], Not arrived), (Action: east, Position: [5, 155], Not arrived), (Action: east, Position: [5, 156], Not arrived), (Action: stay, Position: [5, 157], Arrived), (Action: stay, Position: [5, 157], Arrived), (Action: stay, Position: [5, 157], Arrived), (Action: stay, Position: [5, 157], Arrived), (Action: stay, Position: [5, 157], Arrived), (Action: stay, Position: [5, 157], Arrived)\n",
      "Agent 49 (Goal: [35, 4]): (Action: west, Position: [35, 18], Not arrived), (Action: west, Position: [35, 17], Not arrived), (Action: west, Position: [35, 16], Not arrived), (Action: west, Position: [35, 15], Not arrived), (Action: west, Position: [35, 14], Not arrived), (Action: west, Position: [35, 13], Not arrived), (Action: west, Position: [35, 12], Not arrived), (Action: west, Position: [35, 11], Not arrived), (Action: west, Position: [35, 10], Not arrived), (Action: west, Position: [35, 9], Not arrived), (Action: west, Position: [35, 8], Not arrived), (Action: west, Position: [35, 7], Not arrived), (Action: west, Position: [35, 6], Not arrived), (Action: west, Position: [35, 5], Not arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived), (Action: stay, Position: [35, 4], Arrived)\n",
      "Agent 48 (Goal: [34, 38]): (Action: south, Position: [33, 25], Not arrived), (Action: east, Position: [34, 25], Not arrived), (Action: east, Position: [34, 26], Not arrived), (Action: east, Position: [34, 27], Not arrived), (Action: east, Position: [34, 28], Not arrived), (Action: east, Position: [34, 29], Not arrived), (Action: east, Position: [34, 30], Not arrived), (Action: east, Position: [34, 31], Not arrived), (Action: east, Position: [34, 32], Not arrived), (Action: east, Position: [34, 33], Not arrived), (Action: east, Position: [34, 34], Not arrived), (Action: east, Position: [34, 35], Not arrived), (Action: east, Position: [34, 36], Not arrived), (Action: east, Position: [34, 37], Not arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived), (Action: stay, Position: [34, 38], Arrived)\n",
      "\n",
      "[\n",
      "  {\"agent_id\": [24], \"deadlock\": \"no\"},\n",
      "  {\"agent_id\": [49], \"deadlock\": \"no\"},\n",
      "  {\"agent_id\": [48], \"deadlock\": \"no\"}\n",
      "]\n",
      "Extracted JSON: [{'agent_id': [24], 'deadlock': 'no'}, {'agent_id': [49], 'deadlock': 'no'}, {'agent_id': [48], 'deadlock': 'no'}]\n"
     ]
    }
   ],
   "source": [
    "test_model(128000)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
