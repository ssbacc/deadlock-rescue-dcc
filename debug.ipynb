{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d521989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from typing import Tuple, Union\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from environment import Environment\n",
    "from model import Network\n",
    "import config\n",
    "import copy\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "detection_interval = 4\n",
    "resolution_interval = 16\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c15f48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수들\n",
    "directiondict = {\n",
    "    'stay': 4, 'north': 0, 'south': 1, 'west': 2, 'east': 3\n",
    "}\n",
    "reverse_directiondict = {v: k for k, v in directiondict.items()}\n",
    "\n",
    "def get_possible_directions(obs, obs_agents, agents_pos, agent_idx, agents_not_exchangeable, forbidden_positions):\n",
    "    directions = []\n",
    "    directions_pushed_agents = []\n",
    "    if obs[0][agent_idx][1][3, 4] == 0:\n",
    "        directions.append('north')\n",
    "    if obs[0][agent_idx][1][5, 4] == 0:\n",
    "        directions.append('south')\n",
    "    if obs[0][agent_idx][1][4, 3] == 0:\n",
    "        directions.append('west')\n",
    "    if obs[0][agent_idx][1][4, 5] == 0:\n",
    "        directions.append('east')\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1, [agents_pos[agent_idx][0]-1, agents_pos[agent_idx][1]]),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1, [agents_pos[agent_idx][0]+1, agents_pos[agent_idx][1]]),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]-1]),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]+1])\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if agent_value in agents_not_exchangeable or coordinate in [pos.tolist() if isinstance(pos, np.ndarray) else pos for pos in forbidden_positions]:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def get_possible_directions_super(obs, obs_agents, agents_pos, agent_idx, forbidden_positions):\n",
    "    directions = []\n",
    "    directions_pushed_agents = []\n",
    "    if obs[0][agent_idx][2][4, 4] == 1:\n",
    "        directions.append('north')\n",
    "    if obs[0][agent_idx][3][4, 4] == 1:\n",
    "        directions.append('south')\n",
    "    if obs[0][agent_idx][4][4, 4] == 1:\n",
    "        directions.append('west')\n",
    "    if obs[0][agent_idx][5][4, 4] == 1:\n",
    "        directions.append('east')\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1, [agents_pos[agent_idx][0]-1, agents_pos[agent_idx][1]]),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1, [agents_pos[agent_idx][0]+1, agents_pos[agent_idx][1]]),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]-1]),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]+1])\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if coordinate in [pos.tolist() if isinstance(pos, np.ndarray) else pos for pos in forbidden_positions]:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def push_recursive(obs, obs_agents, agents_pos, agent_super, forbidden_positions):\n",
    "    relayed_actions = []\n",
    "    agents_not_exchangeable = []\n",
    "\n",
    "    current_agent = agent_super\n",
    "    depth = 0\n",
    "\n",
    "    # 스택에 (현재 에이전트, 남은 방향들, depth) 저장\n",
    "    stack = []\n",
    "\n",
    "    while True:\n",
    "        # 가능한 방향들 계산\n",
    "        if current_agent == agent_super:\n",
    "            possible_directions = get_possible_directions_super(obs, obs_agents, agents_pos, current_agent, forbidden_positions)\n",
    "        else:\n",
    "            possible_directions = get_possible_directions(obs, obs_agents, agents_pos, current_agent, agents_not_exchangeable, forbidden_positions)\n",
    "\n",
    "        while not possible_directions:\n",
    "            if not stack:\n",
    "                # 백트래킹할 곳이 없으면 종료'\n",
    "                return [(agent_super, 'stay')]\n",
    "\n",
    "            # 스택에서 이전 상태로 백트래킹\n",
    "            last_agent, last_possible_directions, last_depth = stack.pop()\n",
    "\n",
    "            # 남은 방향이 있다면 그 중 하나를 선택하고 진행\n",
    "            if last_possible_directions:\n",
    "                relayed_actions = relayed_actions[:last_depth]  # 이전 선택을 지우고 다시 선택\n",
    "                current_agent = last_agent\n",
    "                possible_directions = last_possible_directions\n",
    "                depth = last_depth\n",
    "            else:\n",
    "                # 백트래킹할 방향이 없으면 계속 백트래킹\n",
    "                possible_directions = []\n",
    "\n",
    "        # 랜덤으로 가능한 방향 중 하나 선택\n",
    "        choosen_action = random.choice(possible_directions)\n",
    "        possible_directions.remove(choosen_action)\n",
    "\n",
    "        relayed_actions.append((current_agent, choosen_action[0]))\n",
    "\n",
    "        # 더 이상 밀 에이전트가 없으면 종료\n",
    "        if choosen_action[1] is None:\n",
    "            break\n",
    "\n",
    "        # depth에 따른 agents_not_exchangeable 처리\n",
    "        if depth == 1:\n",
    "            agents_not_exchangeable = []\n",
    "        agents_not_exchangeable.append(current_agent)\n",
    "\n",
    "        # 스택에 현재 상태를 저장\n",
    "        stack.append((current_agent, possible_directions, depth))\n",
    "\n",
    "        # 다음 에이전트를 선택하고 루프를 계속\n",
    "        current_agent = choosen_action[1]\n",
    "        depth += 1\n",
    "\n",
    "    return relayed_actions\n",
    "\n",
    "\n",
    "def get_possible_directions_radiation(obs, obs_agents, agents_pos, center_coordinates, agent_idx, forbidden_positions):\n",
    "    directions = []\n",
    "    directions_pushed_agents = []\n",
    "    if obs[0][agent_idx][1][3, 4] == 0:\n",
    "        directions.append('north')\n",
    "    if obs[0][agent_idx][1][5, 4] == 0:\n",
    "        directions.append('south')\n",
    "    if obs[0][agent_idx][1][4, 3] == 0:\n",
    "        directions.append('west')\n",
    "    if obs[0][agent_idx][1][4, 5] == 0:\n",
    "        directions.append('east')\n",
    "    \n",
    "    row_diff = center_coordinates[0] - obs[2][agent_idx][0]\n",
    "    col_diff = center_coordinates[1] - obs[2][agent_idx][1]\n",
    "\n",
    "    if row_diff < 0:  # 에이전트가 중앙보다 아래에 있으면 북쪽으로 이동 불가\n",
    "        if 'north' in directions:\n",
    "            directions.remove('north')\n",
    "    elif row_diff > 0:  # 에이전트가 중앙보다 위에 있으면 남쪽으로 이동 불가\n",
    "        if 'south' in directions:\n",
    "            directions.remove('south')\n",
    "    if col_diff < 0:  # 에이전트가 중앙보다 오른쪽에 있으면 서쪽으로 이동 불가\n",
    "        if 'west' in directions:\n",
    "            directions.remove('west')\n",
    "    elif col_diff > 0:  # 에이전트가 중앙보다 왼쪽에 있으면 동쪽으로 이동 불가\n",
    "        if 'east' in directions:\n",
    "            directions.remove('east')\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1, [agents_pos[agent_idx][0]-1, agents_pos[agent_idx][1]]),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1, [agents_pos[agent_idx][0]+1, agents_pos[agent_idx][1]]),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]-1]),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]+1])\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if coordinate in [pos.tolist() if isinstance(pos, np.ndarray) else pos for pos in forbidden_positions]:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def push_recursive_radiation(obs, obs_agents, agents_pos, center_coordinates, agent_idx, forbidden_positions):\n",
    "\n",
    "    relayed_actions = []\n",
    "    agents_not_exchangeable = []\n",
    "    \n",
    "    current_agent = agent_idx\n",
    "    depth = 0\n",
    "\n",
    "    # 스택에 (현재 에이전트, 남은 방향들, depth) 저장\n",
    "    stack = []\n",
    "\n",
    "    while True:\n",
    "        # 가능한 방향들 계산\n",
    "        if current_agent == agent_idx:\n",
    "            possible_directions = get_possible_directions_radiation(obs, obs_agents, agents_pos, center_coordinates, current_agent, forbidden_positions)\n",
    "        else:\n",
    "            possible_directions = get_possible_directions(obs, obs_agents, agents_pos, current_agent, agents_not_exchangeable, forbidden_positions)\n",
    "\n",
    "        while not possible_directions:\n",
    "            if not stack:\n",
    "                # 백트래킹할 곳이 없으면 종료'\n",
    "                return [(agent_idx, 'stay')]\n",
    "\n",
    "            # 스택에서 이전 상태로 백트래킹\n",
    "            last_agent, last_possible_directions, last_depth = stack.pop()\n",
    "\n",
    "            # 남은 방향이 있다면 그 중 하나를 선택하고 진행\n",
    "            if last_possible_directions:\n",
    "                relayed_actions = relayed_actions[:last_depth]  # 이전 선택을 지우고 다시 선택\n",
    "                current_agent = last_agent\n",
    "                possible_directions = last_possible_directions\n",
    "                depth = last_depth\n",
    "            else:\n",
    "                # 백트래킹할 방향이 없으면 계속 백트래킹\n",
    "                possible_directions = []\n",
    "\n",
    "        # 랜덤으로 가능한 방향 중 하나 선택\n",
    "        choosen_action = random.choice(possible_directions)\n",
    "        possible_directions.remove(choosen_action)\n",
    "\n",
    "        relayed_actions.append((current_agent, choosen_action[0]))\n",
    "\n",
    "        # 더 이상 밀 에이전트가 없으면 종료\n",
    "        if choosen_action[1] is None:\n",
    "            break\n",
    "\n",
    "        # depth에 따른 agents_not_exchangeable 처리\n",
    "        if depth == 1:\n",
    "            agents_not_exchangeable = []\n",
    "        agents_not_exchangeable.append(current_agent)\n",
    "\n",
    "        # 스택에 현재 상태를 저장\n",
    "        stack.append((current_agent, possible_directions, depth))\n",
    "\n",
    "        # 다음 에이전트를 선택하고 루프를 계속\n",
    "        current_agent = choosen_action[1]\n",
    "        depth += 1\n",
    "\n",
    "    return relayed_actions\n",
    "\n",
    "\n",
    "def get_possible_directions_not_deadlock(obs, obs_agents, agents_pos, agent_idx, agent_action, forbidden_positions):\n",
    "    directions = [agent_action]\n",
    "    directions_pushed_agents = []\n",
    "\n",
    "    direction_conditions = [\n",
    "        ('north', obs_agents[agent_idx][3, 4] - 1, [agents_pos[agent_idx][0]-1, agents_pos[agent_idx][1]]),\n",
    "        ('south', obs_agents[agent_idx][5, 4] - 1, [agents_pos[agent_idx][0]+1, agents_pos[agent_idx][1]]),\n",
    "        ('west', obs_agents[agent_idx][4, 3] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]-1]),\n",
    "        ('east', obs_agents[agent_idx][4, 5] - 1, [agents_pos[agent_idx][0], agents_pos[agent_idx][1]+1])\n",
    "    ]\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if coordinate in [pos.tolist() if isinstance(pos, np.ndarray) else pos for pos in forbidden_positions]:\n",
    "            if direction in directions:\n",
    "                directions.remove(direction)\n",
    "\n",
    "    for direction, agent_value, coordinate in direction_conditions:\n",
    "        if direction in directions:\n",
    "            directions_pushed_agents.append((direction, None if agent_value == -1 else agent_value))\n",
    "\n",
    "    return directions_pushed_agents\n",
    "\n",
    "\n",
    "def push_recursive_not_deadlock(obs, obs_agents, agents_pos, agent_idx, agent_action, forbidden_positions):\n",
    "\n",
    "    relayed_actions = []\n",
    "    agents_not_exchangeable = []\n",
    "    \n",
    "    current_agent = agent_idx\n",
    "    depth = 0\n",
    "\n",
    "    # 스택에 (현재 에이전트, 남은 방향들, depth) 저장\n",
    "    stack = []\n",
    "\n",
    "    while True:\n",
    "        # 가능한 방향들 계산\n",
    "        if current_agent == agent_idx:\n",
    "            possible_directions = get_possible_directions_not_deadlock(obs, obs_agents, agents_pos, current_agent, agent_action, forbidden_positions)\n",
    "        else:\n",
    "            possible_directions = get_possible_directions(obs, obs_agents, agents_pos, current_agent, agents_not_exchangeable, forbidden_positions)\n",
    "\n",
    "        while not possible_directions:\n",
    "            if not stack:\n",
    "                # 백트래킹할 곳이 없으면 종료'\n",
    "                return [(agent_idx, 'stay')]\n",
    "\n",
    "            # 스택에서 이전 상태로 백트래킹\n",
    "            last_agent, last_possible_directions, last_depth = stack.pop()\n",
    "\n",
    "            # 남은 방향이 있다면 그 중 하나를 선택하고 진행\n",
    "            if last_possible_directions:\n",
    "                relayed_actions = relayed_actions[:last_depth]  # 이전 선택을 지우고 다시 선택\n",
    "                current_agent = last_agent\n",
    "                possible_directions = last_possible_directions\n",
    "                depth = last_depth\n",
    "            else:\n",
    "                # 백트래킹할 방향이 없으면 계속 백트래킹\n",
    "                possible_directions = []\n",
    "\n",
    "        # 랜덤으로 가능한 방향 중 하나 선택\n",
    "        choosen_action = random.choice(possible_directions)\n",
    "        possible_directions.remove(choosen_action)\n",
    "\n",
    "        relayed_actions.append((current_agent, choosen_action[0]))\n",
    "\n",
    "        # 더 이상 밀 에이전트가 없으면 종료\n",
    "        if choosen_action[1] is None:\n",
    "            break\n",
    "\n",
    "        # depth에 따른 agents_not_exchangeable 처리\n",
    "        if depth == 1:\n",
    "            agents_not_exchangeable = []\n",
    "        agents_not_exchangeable.append(current_agent)\n",
    "\n",
    "        # 스택에 현재 상태를 저장\n",
    "        stack.append((current_agent, possible_directions, depth))\n",
    "\n",
    "        # 다음 에이전트를 선택하고 루프를 계속\n",
    "        current_agent = choosen_action[1]\n",
    "        depth += 1\n",
    "\n",
    "    return relayed_actions\n",
    "\n",
    "\n",
    "def get_randomized_super_agents(agent_groups, env):\n",
    "    super_agents = []\n",
    "    for set_of_agents in agent_groups:\n",
    "        if not set_of_agents:\n",
    "            continue\n",
    "        agent_super = max(set_of_agents, key=lambda i: np.sum(np.abs(env.agents_pos[i] - env.goals_pos[i])))\n",
    "        super_agents.append(agent_super)\n",
    "\n",
    "    if not super_agents:\n",
    "        return []\n",
    "\n",
    "    random.shuffle(super_agents)\n",
    "    return super_agents\n",
    "\n",
    "\n",
    "def update_agent_position(agents_pos, agent_idx, action):\n",
    "    new_position = agents_pos[agent_idx].copy()\n",
    "    if action == 'north':\n",
    "        new_position[0] -= 1\n",
    "    elif action == 'south':\n",
    "        new_position[0] += 1\n",
    "    elif action == 'west':\n",
    "        new_position[1] -= 1\n",
    "    elif action == 'east':\n",
    "        new_position[1] += 1\n",
    "    return new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45c9da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트\n",
    "class gpt4pathfinding:\n",
    "    def detection(self, agents_state):\n",
    "        prompt_text = f\"\"\"\n",
    "                You are given {detection_interval} action logs of agents to detect deadlocks.\n",
    "                \n",
    "                Follow these steps in order:\n",
    "\n",
    "                1. **Classify deadlocks**:\n",
    "                    - Detect agents that are exhibiting deadlock conditions.\n",
    "                    - Deadlock conditions:\n",
    "                        - No movement: The agent's coordinates remain the same for all {detection_interval} logs in the \"Not arrived\" state.\n",
    "                        - Wandering: The agent repeatedly visits the same coordinates during {detection_interval} logs in the \"Not arrived\" state.\n",
    "                    - Not deadlocks:\n",
    "                        - Always \"Arrived\": The agent remains in the \"Arrived\" state throughout.\n",
    "                        - Arrived and stationary: Transitioned from \"Not arrived\" to \"Arrived\" and has stopped moving.\n",
    "                        - Consistent movement: Still \"Not arrived\" but shows regular coordinate changes without revisiting the same location.\n",
    "\n",
    "                2. **Group deadlocked agents**:\n",
    "                    - Group deadlocked agents that are within a 2-Manhattan distance of each other. 2-Manhattan distance means that the sum of the absolute differences between the x-coordinates and y-coordinates of two agents is 2 or less.\n",
    "                    - If a deadlocked agent is within a 2-Manhattan distance of **another agent that has already arrived, include them in the same group**, as these agents can still cause or experience deadlocks. **This includes cases where deadlocked agents are near arrived agents.**\n",
    "\n",
    "                3. **Provide solutions**:\n",
    "                    - Use the \"leader\" method for independently deadlocked agents or if any agent in the group has a goal more than 8 units away in Manhattan distance.\n",
    "                    - Use the \"radiation\" method if all agents in the group are close to their goals (less than 8 units), are deadlocked due to nearby agents, and are likely to experience repeated deadlocks.\n",
    "                    - If a deadlocked agent is near an arrived agent, check their goals and apply the \"radiation\" method if needed, as this can cause performance issues.\n",
    "\n",
    "                Rules:\n",
    "                - Return \"[]\" if no deadlocks are found.\n",
    "                - Ensure no duplicate agents.\n",
    "                - Penalties apply for trivial or non-deadlock cases.\n",
    "                \n",
    "                Below are the {detection_interval} action logs of agents.\n",
    "\n",
    "                {agents_state}\n",
    "\n",
    "                Do not generate a description or explanation.\n",
    "\n",
    "                Provide the agent group status in this JSON format:\n",
    "                {{\n",
    "                    \"agent_id\": [Agent IDs in the same group],\n",
    "                    \"solution\": \"leader\" or \"radiation\"\n",
    "                }}\n",
    "\n",
    "                EXAMPLE 1:\n",
    "                [\n",
    "                    {{\"agent_id\": [1, 24], \"solution\": \"leader\"}},\n",
    "                    {{\"agent_id\": [4, 5], \"solution\": \"radiation\"}}\n",
    "                ]\n",
    "\n",
    "                EXAMPLE 2:\n",
    "                []\n",
    "\n",
    "                EXAMPLE 3:\n",
    "                [\n",
    "                    {{\"agent_id\": [8], \"solution\": \"leader\"}}\n",
    "                ]\n",
    "                \"\"\"\n",
    "\n",
    "        prompt_history.append(prompt_text)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are the manager responsible for detecting whether agents are deadlocked in a MAPF problem. You can infer each agent's state based on their behavior.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "pathfinder = gpt4pathfinding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adf6f06d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(config.test_seed)\n",
    "np.random.seed(config.test_seed)\n",
    "random.seed(config.test_seed)\n",
    "DEVICE = torch.device('cpu')\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4af69b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test(test_env_settings: Tuple = config.test_env_settings, num_test_cases: int = config.num_test_cases):\n",
    "    '''\n",
    "    create test set\n",
    "    '''\n",
    "\n",
    "    for map_length, num_agents, density in test_env_settings:\n",
    "\n",
    "        name = f'./test_set/{map_length}length_{num_agents}agents_{density}density.pth'\n",
    "        print(f'-----{map_length}length {num_agents}agents {density}density-----')\n",
    "\n",
    "        tests = []\n",
    "\n",
    "        env = Environment(fix_density=density, num_agents=num_agents, map_length=map_length)\n",
    "\n",
    "        for _ in tqdm(range(num_test_cases)):\n",
    "            tests.append((np.copy(env.map), np.copy(env.agents_pos), np.copy(env.goals_pos)))\n",
    "            env.reset(num_agents=num_agents, map_length=map_length)\n",
    "        print()\n",
    "\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump(tests, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c11a84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_test():\n",
    "    env = Environment()\n",
    "    network = Network()\n",
    "    network.eval()\n",
    "    obs, last_act, pos = env.observe()\n",
    "    network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(pos.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b34b1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_range: Union[int, tuple], test_set=config.test_env_settings):\n",
    "    '''\n",
    "    test model in 'saved_models' folder\n",
    "    '''\n",
    "    network = Network()\n",
    "    network.eval()\n",
    "    network.to(DEVICE)\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count()//2)\n",
    "\n",
    "    if isinstance(model_range, int):\n",
    "        state_dict = torch.load(os.path.join(config.save_path, f'{model_range}.pth'), map_location=DEVICE)\n",
    "        network.load_state_dict(state_dict)\n",
    "        network.eval()\n",
    "        network.share_memory()\n",
    "\n",
    "        \n",
    "        print(f'----------test model {model_range}----------')\n",
    "\n",
    "        instance_id = 0\n",
    "\n",
    "        for case in test_set:\n",
    "            print(f\"test set: {case[0]} env {case[1]} agents\")\n",
    "            with open('./test_set/{}_{}agents.pth'.format(case[0], case[1]), 'rb') as f:\n",
    "                tests = pickle.load(f)\n",
    "\n",
    "            test = tests[0]\n",
    "            ret = test_one_case((test, network, instance_id))\n",
    "\n",
    "            success, steps, num_comm = ret\n",
    "\n",
    "            # instance_id_base = instance_id\n",
    "            # tests = [(test, network, instance_id_base + i) for i, test in enumerate(tests)]\n",
    "            # ret = pool.map(test_one_case, tests)\n",
    "\n",
    "            # success, steps, num_comm = zip(*ret)\n",
    "\n",
    "            # print(\"success rate: {:.2f}%\".format(sum(success)/len(success)*100))\n",
    "            # print(\"average step: {}\".format(sum(steps)/len(steps)))\n",
    "            # print(\"communication times: {}\".format(sum(num_comm)/len(num_comm)))\n",
    "            # print()\n",
    "\n",
    "            instance_id += len(tests)\n",
    "\n",
    "    elif isinstance(model_range, tuple):\n",
    "\n",
    "        for model_name in range(model_range[0], model_range[1]+1, config.save_interval):\n",
    "            state_dict = torch.load(os.path.join(config.save_path, f'{model_name}.pth'), map_location=DEVICE)\n",
    "            network.load_state_dict(state_dict)\n",
    "            network.eval()\n",
    "            network.share_memory()\n",
    "\n",
    "\n",
    "            print(f'----------test model {model_name}----------')\n",
    "\n",
    "            instance_id = 0\n",
    "\n",
    "            for case in test_set:\n",
    "                print(f\"test set: {case[0]} length {case[1]} agents {case[2]} density\")\n",
    "                with open(f'./test_set/{case[0]}length_{case[1]}agents_{case[2]}density.pth', 'rb') as f:\n",
    "                    tests = pickle.load(f)\n",
    "\n",
    "                test = tests[0]\n",
    "                ret = test_one_case((test, network, instance_id))\n",
    "\n",
    "                success, steps, num_comm = ret\n",
    "\n",
    "                # instance_id_base = instance_id\n",
    "                # tests = [(test, network, instance_id_base + i) for i, test in enumerate(tests)]\n",
    "                # ret = pool.map(test_one_case, tests)\n",
    "\n",
    "                # success, steps, num_comm = zip(*ret)\n",
    "\n",
    "                # print(\"success rate: {:.2f}%\".format(sum(success)/len(success)*100))\n",
    "                # print(\"average step: {}\".format(sum(steps)/len(steps)))\n",
    "                # print(\"communication times: {}\".format(sum(num_comm)/len(num_comm)))\n",
    "                # print()\n",
    "\n",
    "                instance_id += 1\n",
    "\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52ab1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_case(args):\n",
    "\n",
    "    env_set, network, instance_id = args\n",
    "\n",
    "    env = Environment()\n",
    "    env.load(np.array(env_set[0]), np.array(env_set[1]), np.array(env_set[2]))\n",
    "    obs, last_act, pos = env.observe()\n",
    "    \n",
    "    done = False\n",
    "    network.reset()\n",
    "\n",
    "    num_agents = len(env_set[1])\n",
    "\n",
    "    step = 0\n",
    "    num_comm = 0\n",
    "\n",
    "    while not done and env.steps < config.max_episode_length:\n",
    "        env_copy = copy.deepcopy(env)\n",
    "        plan = []\n",
    "        not_arrived = set()\n",
    "        sim_obs, sim_last_act, sim_pos = env_copy.observe()\n",
    "        sim_done = False\n",
    "        for _ in range(resolution_interval):\n",
    "            if env_copy.steps >= config.max_episode_length or sim_done:\n",
    "                break\n",
    "            actions, _, _, _, comm_mask = network.step(torch.as_tensor(sim_obs.astype(np.float32)).to(DEVICE), \n",
    "                                                        torch.as_tensor(sim_last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                        torch.as_tensor(sim_pos.astype(int)))\n",
    "            plan.append((actions, comm_mask, copy.deepcopy(sim_pos)))\n",
    "            (sim_obs, sim_last_act, sim_pos), _, sim_done, _ = env_copy.step(actions)\n",
    "            for i in range(num_agents):\n",
    "                if not np.array_equal(env_copy.agents_pos[i], env_copy.goals_pos[i]):\n",
    "                    not_arrived.add(i)\n",
    "        not_arrived = list(not_arrived)\n",
    "\n",
    "        observations = env.observe_agents()\n",
    "        FOV_agents = [\n",
    "            [*(observations[i][observations[i] != 0] - 1).tolist(), i] if np.any(observations[i]) else [i]\n",
    "            for i in not_arrived\n",
    "        ]\n",
    "        agents_to_prompt = list({agent for agent_list in FOV_agents for agent in agent_list})\n",
    "        agents_to_prompt = sorted(agents_to_prompt)\n",
    "\n",
    "        planned_steps_dict = {i: [] for i in agents_to_prompt}\n",
    "        goal_logged = {i: False for i in agents_to_prompt}\n",
    "        for i in plan[:detection_interval]:\n",
    "            actions, _, positions = i\n",
    "            for agent_idx in agents_to_prompt:\n",
    "                position = positions[agent_idx]\n",
    "                # 목표 위치와 현재 위치를 비교하여 도달 여부 판단\n",
    "                arrived_status = \"Arrived\" if np.array_equal(position, env.goals_pos[agent_idx]) else \"Not arrived\"\n",
    "                if not goal_logged[agent_idx]:\n",
    "                    planned_steps_dict[agent_idx].append(\n",
    "                        f\"(Position: [{position[0]}, {position[1]}], {arrived_status})\"\n",
    "                    )\n",
    "                    goal_logged[agent_idx] = True\n",
    "                else:\n",
    "                    planned_steps_dict[agent_idx].append(\n",
    "                        f\"(Position: [{position[0]}, {position[1]}], {arrived_status})\"\n",
    "                    )\n",
    "        agents_state = \"\"\n",
    "        for agent_idx in planned_steps_dict:\n",
    "            agent_goal = f\" (Goal: [{env.goals_pos[agent_idx][0]}, {env.goals_pos[agent_idx][1]}])\"\n",
    "            agent_log = \", \".join(planned_steps_dict[agent_idx])\n",
    "            agents_state += f\"Agent {agent_idx}{agent_goal}: {agent_log}\\n\"\n",
    "\n",
    "        gpt4_response = pathfinder.detection(agents_state)\n",
    "        response_text = gpt4_response\n",
    "        try:\n",
    "            start_idx = response_text.index('[')\n",
    "            end_idx = response_text.rindex(']') + 1\n",
    "            json_part = response_text[start_idx:end_idx]\n",
    "            json_data = json.loads(json_part)\n",
    "            print(\"Extracted JSON:\", json_data)\n",
    "        except:\n",
    "            print(\"JSON 부분을 찾을 수 없으므로 deadlock이 없다고 가정합니다.\")\n",
    "            json_data = []\n",
    "\n",
    "        deadlock_exists = len(json_data) > 0\n",
    "        \n",
    "        if not deadlock_exists:\n",
    "            for actions, comm_mask, _ in plan:\n",
    "                if env.steps >= config.max_episode_length or done:\n",
    "                    break\n",
    "                (obs, last_act, pos), _, done, _ = env.step(actions)\n",
    "                env.save_frame(step, instance_id)\n",
    "                step += 1\n",
    "                num_comm += np.sum(comm_mask)\n",
    "        else:\n",
    "            leader_agents = []\n",
    "            radiation_agents = []\n",
    "\n",
    "            if isinstance(json_data, list):\n",
    "                for item in json_data:\n",
    "                    if isinstance(item, dict):\n",
    "                        if item.get('solution') == 'leader':\n",
    "                            leader_agents.append(item['agent_id'])\n",
    "                        elif item.get('solution') == 'radiation':\n",
    "                            radiation_agents.append(item['agent_id'])\n",
    "                            \n",
    "            leader_agents = [[agent for agent in group if agent < num_agents] for group in leader_agents]\n",
    "            radiation_agents = [[agent for agent in group if agent < num_agents] for group in radiation_agents]\n",
    "\n",
    "            deadlocked_agents = set()\n",
    "            for group in leader_agents + radiation_agents:\n",
    "                deadlocked_agents.update(group)\n",
    "\n",
    "            all_agents = set(range(num_agents))\n",
    "            no_deadlock_agents = list(all_agents - deadlocked_agents)\n",
    "\n",
    "            sorted_leader_agents = get_randomized_super_agents(leader_agents, env)\n",
    "            random.shuffle(radiation_agents)\n",
    "            random.shuffle(no_deadlock_agents)\n",
    "\n",
    "            for _ in range(resolution_interval):\n",
    "                if env.steps >= config.max_episode_length or done:\n",
    "                    break\n",
    "                obs_agents = env.observe_agents()\n",
    "                observation = env.observe()\n",
    "                agents_pos = env.agents_pos\n",
    "\n",
    "                manual_actions = [4 for _ in range(num_agents)]\n",
    "                ml_planned_actions, _, _, _, _ = network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE), \n",
    "                                                    torch.as_tensor(pos.astype(int)))\n",
    "                \n",
    "                forbidden_positions = []\n",
    "                agents_moved = []\n",
    "\n",
    "                for super_agent in sorted_leader_agents:\n",
    "                    if super_agent in agents_moved:\n",
    "                        continue\n",
    "                    for relayed_action in push_recursive(observation, obs_agents, agents_pos, super_agent, forbidden_positions):\n",
    "                        agent_idx = relayed_action[0]\n",
    "                        action = relayed_action[1]\n",
    "                        manual_actions[agent_idx] = directiondict[action]\n",
    "                        agents_moved.append(agent_idx)\n",
    "                        new_position = update_agent_position(agents_pos, agent_idx, action)\n",
    "                        forbidden_positions.append(new_position)\n",
    "\n",
    "                for set_of_agents in radiation_agents:\n",
    "                    x_values = []\n",
    "                    y_values = []\n",
    "                    random.shuffle(set_of_agents)\n",
    "                    for agent_idx in set_of_agents:\n",
    "                        x_values.append(observation[2][agent_idx][0])\n",
    "                        y_values.append(observation[2][agent_idx][1])\n",
    "                    if len(x_values) == 0 or len(y_values) == 0:\n",
    "                        continue\n",
    "                    avg_x = sum(x_values) / len(x_values)\n",
    "                    avg_y = sum(y_values) / len(y_values)\n",
    "                    average_position = (avg_x, avg_y)\n",
    "\n",
    "                    for radiation_agent in set_of_agents:\n",
    "                        if radiation_agent in agents_moved:\n",
    "                            continue\n",
    "                        for relayed_action in push_recursive_radiation(observation, obs_agents, agents_pos, average_position, radiation_agent, forbidden_positions):\n",
    "                            agent_idx = relayed_action[0]\n",
    "                            action = relayed_action[1]\n",
    "                            manual_actions[agent_idx] = directiondict[action]\n",
    "                            agents_moved.append(agent_idx)\n",
    "                            new_position = update_agent_position(agents_pos, agent_idx, action)\n",
    "                            forbidden_positions.append(new_position)\n",
    "\n",
    "                for no_deadlock_agent in no_deadlock_agents:\n",
    "                    if no_deadlock_agent in agents_moved:\n",
    "                        continue\n",
    "                    for relayed_action in push_recursive_not_deadlock(observation, obs_agents, agents_pos, no_deadlock_agent, reverse_directiondict[ml_planned_actions[no_deadlock_agent]], forbidden_positions):\n",
    "                        agent_idx = relayed_action[0]\n",
    "                        action = relayed_action[1]\n",
    "                        manual_actions[agent_idx] = directiondict[action]\n",
    "                        agents_moved.append(agent_idx)\n",
    "                        new_position = update_agent_position(agents_pos, agent_idx, action)\n",
    "                        forbidden_positions.append(new_position)\n",
    "      \n",
    "                (obs, last_act, pos), _, done, _ = env.step(manual_actions)\n",
    "                env.save_frame(step, instance_id)\n",
    "                step += 1\n",
    "                num_comm += np.sum(comm_mask)\n",
    "\n",
    "    return np.array_equal(env.agents_pos, env.goals_pos), step, num_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7da42859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_8404\\1337376553.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(config.save_path, f'{model_range}.pth'), map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------test model 128000----------\n",
      "test set: warehouse env 64 agents\n",
      "0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "Extracted JSON: []\n",
      "16\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "Extracted JSON: []\n",
      "32\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "Extracted JSON: []\n",
      "48\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "Extracted JSON: [{'agent_id': [21, 36], 'solution': 'radiation'}, {'agent_id': [1, 41, 29], 'solution': 'leader'}]\n",
      "64\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 29, 30, 32, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 52, 54, 56, 57, 58, 61, 62, 63]\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 52, 53, 54, 56, 57, 58, 61, 62, 63]\n",
      "Extracted JSON: [{'agent_id': [7, 52], 'solution': 'radiation'}, {'agent_id': [15], 'solution': 'leader'}, {'agent_id': [24], 'solution': 'leader'}]\n",
      "80\n",
      "[0, 1, 2, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 29, 30, 32, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 52, 54, 56, 57, 58, 63]\n",
      "[0, 1, 2, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 29, 30, 32, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 52, 54, 55, 56, 57, 58, 61, 63]\n",
      "Extracted JSON: [{'agent_id': [5], 'solution': 'leader'}, {'agent_id': [54], 'solution': 'leader'}]\n",
      "96\n",
      "[0, 2, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 27, 29, 30, 32, 36, 38, 40, 41, 42, 44, 45, 47, 48, 49, 52, 56, 57]\n",
      "[0, 2, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 27, 29, 30, 32, 34, 36, 38, 40, 41, 42, 44, 45, 47, 48, 49, 52, 54, 56, 57, 62]\n",
      "Extracted JSON: [{'agent_id': [0], 'solution': 'leader'}, {'agent_id': [5, 17], 'solution': 'leader'}, {'agent_id': [6, 8, 18], 'solution': 'radiation'}, {'agent_id': [7, 52], 'solution': 'radiation'}, {'agent_id': [15], 'solution': 'leader'}, {'agent_id': [30], 'solution': 'leader'}]\n",
      "112\n",
      "[0, 2, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 27, 29, 32, 38, 40, 41, 42, 43, 44, 45, 47, 49, 52, 54, 57, 62]\n",
      "[0, 2, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 27, 29, 30, 32, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 52, 54, 55, 57, 62]\n",
      "Extracted JSON: [{'agent_id': [5], 'solution': 'leader'}, {'agent_id': [7, 17], 'solution': 'radiation'}, {'agent_id': [23, 32], 'solution': 'radiation'}, {'agent_id': [54], 'solution': 'leader'}]\n",
      "128\n",
      "[5, 6, 7, 8, 12, 13, 15, 17, 18, 21, 22, 23, 24, 29, 30, 32, 38, 40, 41, 42, 44, 47, 48, 49, 52, 57, 63]\n",
      "[0, 1, 3, 5, 6, 7, 8, 12, 13, 15, 17, 18, 20, 21, 22, 23, 24, 29, 30, 32, 38, 39, 40, 41, 42, 44, 47, 48, 49, 52, 54, 57, 63]\n",
      "Extracted JSON: [{'agent_id': [15, 39], 'solution': 'radiation'}, {'agent_id': [30, 41], 'solution': 'radiation'}, {'agent_id': [49, 48], 'solution': 'radiation'}]\n",
      "144\n",
      "[0, 6, 7, 8, 12, 15, 17, 18, 21, 22, 23, 24, 30, 32, 39, 40, 41, 42, 48, 49, 52, 57, 63]\n",
      "[0, 6, 7, 8, 12, 15, 17, 18, 21, 22, 23, 24, 30, 32, 39, 40, 41, 42, 48, 49, 52, 54, 57, 63]\n",
      "Extracted JSON: [{'agent_id': [7, 63, 52], 'solution': 'radiation'}, {'agent_id': [23, 32], 'solution': 'radiation'}]\n",
      "160\n",
      "[0, 32, 6, 7, 8, 42, 15, 49, 19, 52, 21, 23, 24, 57, 30, 63]\n",
      "[0, 6, 7, 8, 15, 16, 17, 18, 19, 21, 23, 24, 26, 30, 32, 42, 48, 49, 52, 54, 57, 61, 63]\n",
      "Extracted JSON: [{'agent_id': [7, 19], 'solution': 'radiation'}, {'agent_id': [15], 'solution': 'leader'}, {'agent_id': [8, 18, 30], 'solution': 'radiation'}, {'agent_id': [49], 'solution': 'leader'}, {'agent_id': [32], 'solution': 'leader'}]\n",
      "176\n",
      "[32, 6, 7, 8, 41, 42, 49, 18, 19, 52, 21, 54, 23, 25, 30, 63]\n",
      "[6, 7, 8, 15, 18, 19, 21, 23, 25, 28, 30, 32, 41, 42, 49, 52, 53, 54, 63]\n",
      "Extracted JSON: [{'agent_id': [8, 18, 25], 'solution': 'radiation'}, {'agent_id': [23, 32], 'solution': 'leader'}]\n",
      "192\n",
      "[32, 6, 7, 8, 41, 42, 49, 18, 52, 21, 23, 25]\n",
      "[0, 6, 7, 8, 18, 21, 23, 25, 32, 33, 35, 41, 42, 49, 52]\n",
      "Extracted JSON: []\n",
      "208\n",
      "[32, 6, 7, 8, 42, 18, 52, 21, 25]\n",
      "[6, 7, 8, 18, 21, 25, 32, 42, 52]\n",
      "Extracted JSON: [{'agent_id': [52], 'solution': 'leader'}]\n",
      "224\n",
      "[32, 7, 8, 42, 52, 21, 54, 30]\n",
      "[4, 7, 8, 17, 18, 20, 21, 30, 32, 38, 42, 52, 54]\n",
      "Extracted JSON: [{'agent_id': [8, 30, 54], 'solution': 'radiation'}, {'agent_id': [32], 'solution': 'leader'}]\n",
      "240\n",
      "[32, 38, 8, 42, 52, 54, 30]\n",
      "[8, 25, 26, 28, 29, 30, 32, 38, 42, 52, 54, 59]\n",
      "Extracted JSON: []\n",
      "256\n",
      "[8, 54, 30]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: [{'agent_id': [8, 54], 'solution': 'radiation'}]\n",
      "272\n",
      "[8, 18, 54]\n",
      "[8, 11, 18, 25, 28, 30, 54]\n",
      "Extracted JSON: []\n",
      "288\n",
      "[8, 18, 54, 30]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: [{'agent_id': [54, 18, 8], 'solution': 'radiation'}]\n",
      "304\n",
      "[8, 18, 54, 25, 28]\n",
      "[8, 11, 18, 25, 28, 54]\n",
      "Extracted JSON: [{'agent_id': [25, 54], 'solution': 'radiation'}]\n",
      "320\n",
      "[8, 25, 54, 30]\n",
      "[8, 16, 18, 25, 30, 54, 61]\n",
      "Extracted JSON: []\n",
      "336\n",
      "[25, 54]\n",
      "[8, 18, 25, 30, 54]\n",
      "Extracted JSON: []\n",
      "352\n",
      "[54]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: [{'agent_id': [18, 54], 'solution': 'radiation'}]\n",
      "368\n",
      "[54, 25, 18, 30]\n",
      "[18, 25, 30, 54]\n",
      "Extracted JSON: [{'agent_id': [25, 54], 'solution': 'leader'}]\n",
      "384\n",
      "[54, 25, 30]\n",
      "[8, 18, 25, 30, 54]\n",
      "Extracted JSON: [{'agent_id': [30, 54], 'solution': 'radiation'}]\n",
      "400\n",
      "[8, 54, 18, 30]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: []\n",
      "416\n",
      "[54, 8, 18, 30]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: []\n",
      "432\n",
      "[54]\n",
      "[8, 18, 54]\n",
      "Extracted JSON: []\n",
      "448\n",
      "[54]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: []\n",
      "464\n",
      "[8, 18, 54]\n",
      "[8, 18, 30, 54]\n",
      "Extracted JSON: [{'agent_id': [54, 18], 'solution': 'radiation'}]\n",
      "480\n",
      "[18, 54]\n",
      "[18, 25, 54]\n",
      "Extracted JSON: []\n",
      "496\n",
      "[]\n",
      "[]\n",
      "Extracted JSON: []\n"
     ]
    }
   ],
   "source": [
    "prompt_history = []\n",
    "\n",
    "test_model(128000)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
